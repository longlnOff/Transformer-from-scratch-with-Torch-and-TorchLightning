{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0, 3, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = einops.rearrange(torch.arange(0, 100), '(a b) -> a b', a=5, b=20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "div_term = torch.exp(torch.arange(0, 100, 2) * (-torch.log(torch.tensor(10000.0)) / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.ones(1, 6, 100)\n",
    "x = torch.zeros(32, 6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + pe[:, :x.shape[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.tensor(100, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "assert isinstance(a, float), 'wrong type hyperparameter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def calculate_even_pos(pos, i):\n",
    "    return torch.sin(pos / (10000 ** (2 * i / 512)))\n",
    "\n",
    "def calculate_odd_pos(pos, i):\n",
    "    return torch.cos(pos / (10000 ** (2 * i / 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(12, 512)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0366e+00, 1.0746e+00, 1.1140e+00, 1.1548e+00, 1.1971e+00,\n",
       "        1.2409e+00, 1.2864e+00, 1.3335e+00, 1.3824e+00, 1.4330e+00, 1.4855e+00,\n",
       "        1.5399e+00, 1.5963e+00, 1.6548e+00, 1.7154e+00, 1.7783e+00, 1.8434e+00,\n",
       "        1.9110e+00, 1.9810e+00, 2.0535e+00, 2.1288e+00, 2.2067e+00, 2.2876e+00,\n",
       "        2.3714e+00, 2.4582e+00, 2.5483e+00, 2.6416e+00, 2.7384e+00, 2.8387e+00,\n",
       "        2.9427e+00, 3.0505e+00, 3.1623e+00, 3.2781e+00, 3.3982e+00, 3.5227e+00,\n",
       "        3.6517e+00, 3.7855e+00, 3.9242e+00, 4.0679e+00, 4.2170e+00, 4.3714e+00,\n",
       "        4.5316e+00, 4.6976e+00, 4.8697e+00, 5.0481e+00, 5.2330e+00, 5.4247e+00,\n",
       "        5.6234e+00, 5.8294e+00, 6.0430e+00, 6.2643e+00, 6.4938e+00, 6.7317e+00,\n",
       "        6.9783e+00, 7.2339e+00, 7.4989e+00, 7.7737e+00, 8.0584e+00, 8.3536e+00,\n",
       "        8.6596e+00, 8.9769e+00, 9.3057e+00, 9.6466e+00, 1.0000e+01, 1.0366e+01,\n",
       "        1.0746e+01, 1.1140e+01, 1.1548e+01, 1.1971e+01, 1.2409e+01, 1.2864e+01,\n",
       "        1.3335e+01, 1.3824e+01, 1.4330e+01, 1.4855e+01, 1.5399e+01, 1.5963e+01,\n",
       "        1.6548e+01, 1.7154e+01, 1.7783e+01, 1.8434e+01, 1.9110e+01, 1.9810e+01,\n",
       "        2.0535e+01, 2.1288e+01, 2.2067e+01, 2.2876e+01, 2.3714e+01, 2.4582e+01,\n",
       "        2.5483e+01, 2.6416e+01, 2.7384e+01, 2.8387e+01, 2.9427e+01, 3.0505e+01,\n",
       "        3.1623e+01, 3.2781e+01, 3.3982e+01, 3.5227e+01, 3.6517e+01, 3.7855e+01,\n",
       "        3.9242e+01, 4.0679e+01, 4.2170e+01, 4.3714e+01, 4.5316e+01, 4.6976e+01,\n",
       "        4.8697e+01, 5.0481e+01, 5.2330e+01, 5.4247e+01, 5.6234e+01, 5.8294e+01,\n",
       "        6.0430e+01, 6.2643e+01, 6.4938e+01, 6.7317e+01, 6.9783e+01, 7.2339e+01,\n",
       "        7.4989e+01, 7.7737e+01, 8.0584e+01, 8.3536e+01, 8.6596e+01, 8.9769e+01,\n",
       "        9.3057e+01, 9.6466e+01, 1.0000e+02, 1.0366e+02, 1.0746e+02, 1.1140e+02,\n",
       "        1.1548e+02, 1.1971e+02, 1.2409e+02, 1.2864e+02, 1.3335e+02, 1.3824e+02,\n",
       "        1.4330e+02, 1.4855e+02, 1.5399e+02, 1.5963e+02, 1.6548e+02, 1.7154e+02,\n",
       "        1.7783e+02, 1.8434e+02, 1.9110e+02, 1.9810e+02, 2.0535e+02, 2.1288e+02,\n",
       "        2.2067e+02, 2.2876e+02, 2.3714e+02, 2.4582e+02, 2.5483e+02, 2.6416e+02,\n",
       "        2.7384e+02, 2.8387e+02, 2.9427e+02, 3.0505e+02, 3.1623e+02, 3.2781e+02,\n",
       "        3.3982e+02, 3.5227e+02, 3.6517e+02, 3.7855e+02, 3.9242e+02, 4.0679e+02,\n",
       "        4.2170e+02, 4.3714e+02, 4.5316e+02, 4.6976e+02, 4.8697e+02, 5.0481e+02,\n",
       "        5.2330e+02, 5.4247e+02, 5.6234e+02, 5.8294e+02, 6.0430e+02, 6.2643e+02,\n",
       "        6.4938e+02, 6.7317e+02, 6.9783e+02, 7.2339e+02, 7.4989e+02, 7.7737e+02,\n",
       "        8.0584e+02, 8.3536e+02, 8.6596e+02, 8.9769e+02, 9.3057e+02, 9.6466e+02,\n",
       "        1.0000e+03, 1.0366e+03, 1.0746e+03, 1.1140e+03, 1.1548e+03, 1.1971e+03,\n",
       "        1.2409e+03, 1.2864e+03, 1.3335e+03, 1.3824e+03, 1.4330e+03, 1.4855e+03,\n",
       "        1.5399e+03, 1.5963e+03, 1.6548e+03, 1.7154e+03, 1.7783e+03, 1.8434e+03,\n",
       "        1.9110e+03, 1.9810e+03, 2.0535e+03, 2.1288e+03, 2.2067e+03, 2.2876e+03,\n",
       "        2.3714e+03, 2.4582e+03, 2.5483e+03, 2.6416e+03, 2.7384e+03, 2.8387e+03,\n",
       "        2.9427e+03, 3.0505e+03, 3.1623e+03, 3.2781e+03, 3.3982e+03, 3.5227e+03,\n",
       "        3.6517e+03, 3.7855e+03, 3.9242e+03, 4.0679e+03, 4.2170e+03, 4.3714e+03,\n",
       "        4.5316e+03, 4.6976e+03, 4.8697e+03, 5.0481e+03, 5.2330e+03, 5.4247e+03,\n",
       "        5.6234e+03, 5.8294e+03, 6.0430e+03, 6.2643e+03, 6.4938e+03, 6.7317e+03,\n",
       "        6.9783e+03, 7.2339e+03, 7.4989e+03, 7.7737e+03, 8.0584e+03, 8.3536e+03,\n",
       "        8.6596e+03, 8.9769e+03, 9.3057e+03, 9.6466e+03])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 512)\n",
    "10000 ** (2 * ( a[::2] // 2) / 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = 1 /10000. ** (2 * ( a[::2] // 2) / 512)\n",
    "div_term = torch.exp(torch.arange(0, 512, 2).float() * (-math.log(10000.0) / 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(0, 12, dtype=int)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[pos, 0::2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, 512, 2).float() * (-math.log(10000.0) / 512))\n",
    "pos = torch.arange(0, 12, dtype=int)\n",
    "x[pos, 0::2] = torch.sin(pos.unsqueeze(1).float()@div_term.unsqueeze(0))\n",
    "x[pos, 1::2] = torch.cos(pos.unsqueeze(1).float()@div_term.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(12):\n",
    "#     x[i, 0::2] = torch.sin(i*div_term)\n",
    "#     x[i, 1::2] = torch.cos(i*div_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.4147e-01,  8.6069e-01,  8.7940e-01,  8.9746e-01,  9.1471e-01,\n",
       "         9.3098e-01,  9.4609e-01,  9.5983e-01,  9.7198e-01,  9.8230e-01,\n",
       "         9.9052e-01,  9.9637e-01,  9.9952e-01,  9.9967e-01,  9.9647e-01,\n",
       "         9.8956e-01,  9.7855e-01,  9.6307e-01,  9.4270e-01,  9.1706e-01,\n",
       "         8.8573e-01,  8.4834e-01,  8.0452e-01,  7.5393e-01,  6.9629e-01,\n",
       "         6.3139e-01,  5.5910e-01,  4.7938e-01,  3.9234e-01,  2.9825e-01,\n",
       "         1.9756e-01,  9.0939e-02, -2.0684e-02, -1.3610e-01, -2.5381e-01,\n",
       "        -3.7194e-01, -4.8831e-01, -6.0034e-01, -7.0512e-01, -7.9943e-01,\n",
       "        -8.7977e-01, -9.4244e-01, -9.8370e-01, -9.9989e-01, -9.8766e-01,\n",
       "        -9.4419e-01, -8.6752e-01, -7.5686e-01, -6.1294e-01, -4.3836e-01,\n",
       "        -2.3792e-01, -1.8849e-02,  2.0908e-01,  4.3363e-01,  6.4048e-01,\n",
       "         8.1386e-01,  9.3763e-01,  9.9678e-01,  9.7917e-01,  8.7775e-01,\n",
       "         6.9263e-01,  4.3308e-01,  1.1878e-01, -2.2002e-01, -5.4402e-01,\n",
       "        -8.0847e-01, -9.6904e-01, -9.8963e-01, -8.5135e-01, -5.6094e-01,\n",
       "        -1.5635e-01,  2.9323e-01,  6.9530e-01,  9.5128e-01,  9.8144e-01,\n",
       "         7.5318e-01,  3.0382e-01, -2.5265e-01, -7.4478e-01, -9.9227e-01,\n",
       "        -8.7564e-01, -4.0349e-01,  2.5706e-01,  8.1920e-01,  9.9341e-01,\n",
       "         6.4699e-01, -7.6119e-02, -7.7365e-01, -9.8850e-01, -5.2294e-01,\n",
       "         3.4311e-01,  9.5908e-01,  7.7716e-01, -1.1279e-01, -9.1396e-01,\n",
       "        -7.8990e-01,  2.0538e-01,  9.7896e-01,  5.4420e-01, -6.2054e-01,\n",
       "        -9.2525e-01,  1.5541e-01,  9.9961e-01,  1.6056e-01, -9.7090e-01,\n",
       "        -2.6466e-01,  9.7199e-01,  1.4746e-01, -1.0000e+00,  2.1352e-01,\n",
       "         8.8062e-01, -7.4453e-01, -3.0938e-01,  9.8478e-01, -6.7383e-01,\n",
       "        -1.8739e-01,  8.6001e-01, -9.7430e-01,  6.1943e-01, -8.2691e-02,\n",
       "        -3.9751e-01,  7.1966e-01, -8.8993e-01,  9.5993e-01, -9.7954e-01,\n",
       "         9.7290e-01, -9.2858e-01,  7.9754e-01, -5.0637e-01,  9.2655e-03,\n",
       "         6.0250e-01, -9.9167e-01,  6.8945e-01,  3.2213e-01, -1.0000e+00,\n",
       "         1.6485e-01,  9.8633e-01,  7.1516e-03, -9.3632e-01, -7.8080e-01,\n",
       "        -5.4578e-02,  5.5418e-01,  8.5352e-01,  9.4701e-01,  9.4670e-01,\n",
       "         8.4772e-01,  5.1573e-01, -1.7444e-01, -9.1235e-01, -6.8393e-01,\n",
       "         6.9031e-01,  5.4714e-01, -9.9860e-01,  7.0340e-01, -3.5291e-01,\n",
       "         2.6774e-01, -4.9982e-01,  9.0452e-01, -8.6088e-01, -3.1296e-01,\n",
       "         8.7869e-01,  8.8499e-01,  5.0452e-01,  3.9961e-01,  6.8119e-01,\n",
       "         9.9995e-01,  2.7646e-01, -9.9913e-01,  6.6170e-01, -4.4672e-01,\n",
       "         6.9543e-01, -9.9588e-01, -2.0667e-02,  8.3594e-01,  9.7503e-01,\n",
       "         8.5545e-01,  3.7764e-03, -9.8454e-01,  8.9596e-01, -9.5104e-01,\n",
       "         8.0044e-01,  7.6406e-01,  3.8665e-01,  7.3648e-01,  8.1133e-01,\n",
       "        -9.8400e-01,  9.9972e-01, -2.9657e-01, -8.9803e-01, -7.2316e-01,\n",
       "         6.1359e-01, -1.9149e-01,  8.2688e-01, -9.2509e-02,  1.8208e-01,\n",
       "         9.6125e-01, -9.6973e-01, -1.3784e-01, -8.6459e-03, -9.9637e-01,\n",
       "         9.9642e-01,  7.1364e-02,  4.3165e-01,  4.4866e-01,  5.1937e-01,\n",
       "         3.9809e-01,  7.1888e-01,  1.2793e-01,  1.3754e-01,  6.3801e-01,\n",
       "         7.6088e-01,  9.8337e-01, -8.8028e-01, -9.4839e-01,  9.7258e-01,\n",
       "         4.7397e-01,  5.0439e-01,  9.9864e-01, -4.4841e-01,  4.1777e-01,\n",
       "        -8.6698e-01, -9.5321e-01,  8.1065e-01, -4.1354e-02,  9.6520e-01,\n",
       "        -9.9145e-01, -8.3876e-01, -8.2444e-01,  9.3581e-01,  1.0382e-01,\n",
       "        -3.3393e-01,  4.0607e-01,  8.1196e-01, -9.9669e-01,  9.8660e-01,\n",
       "        -7.8818e-01,  2.0521e-01,  4.5598e-01, -7.8466e-01,  7.4437e-01,\n",
       "        -3.7755e-02, -9.8194e-01, -9.9391e-01, -3.0204e-04, -1.4389e-01,\n",
       "         6.6645e-01, -7.3684e-01,  9.1132e-01,  3.9759e-02,  9.7576e-01,\n",
       "        -2.3451e-01, -1.2977e-01,  9.8776e-01, -9.7369e-01,  3.1766e-01,\n",
       "         9.3731e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(10000 ** (2 * ( a[::2] // 2) / 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
       "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
       "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
       "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
       "        113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139,\n",
       "        141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167,\n",
       "        169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195,\n",
       "        197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223,\n",
       "        225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251,\n",
       "        253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279,\n",
       "        281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307,\n",
       "        309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335,\n",
       "        337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363,\n",
       "        365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389, 391,\n",
       "        393, 395, 397, 399, 401, 403, 405, 407, 409, 411, 413, 415, 417, 419,\n",
       "        421, 423, 425, 427, 429, 431, 433, 435, 437, 439, 441, 443, 445, 447,\n",
       "        449, 451, 453, 455, 457, 459, 461, 463, 465, 467, 469, 471, 473, 475,\n",
       "        477, 479, 481, 483, 485, 487, 489, 491, 493, 495, 497, 499, 501, 503,\n",
       "        505, 507, 509, 511])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1::2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # craete matrix of positional encoding\n",
    "        pe = torch.zeros(self.seq_len, self.d_model) # shape = [seq_len, d_model]\n",
    "\n",
    "        # create position matrix\n",
    "        position = torch.arange(0, self.seq_len, dtype=torch.float32).unsqueeze(1) # shape = [seq_len, 1]\n",
    "        # we calculate positional in log scale for stability\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model)) # shape = [d_model/2]\n",
    "        # calculate positional encoding, apply sin to even index in the array; 2i and apply cos to odd index in the array; 2i+1\n",
    "        # apply sine to even indices in the array; 2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # apply cosine to odd indices in the array; 2i+1\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register buffer to save positional encoding (this is not model parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = [batch_size, seq_len, d_model]\n",
    "        # pe shape = [1, seq_len, d_model]\n",
    "        # add positional encoding to each sequence in batch\n",
    "        # below code will broadcast pe over batch dimension\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(seq_len=12, dropout=0., d_model=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4147e-01, 5.4030e-01, 8.2186e-01, 5.6970e-01, 8.0196e-01, 5.9738e-01,\n",
       "        7.8189e-01, 6.2342e-01, 7.6172e-01, 6.4791e-01, 7.4154e-01, 6.7091e-01,\n",
       "        7.2141e-01, 6.9250e-01, 7.0140e-01, 7.1276e-01, 6.8156e-01, 7.3176e-01,\n",
       "        6.6193e-01, 7.4956e-01, 6.4256e-01, 7.6624e-01, 6.2347e-01, 7.8185e-01,\n",
       "        6.0469e-01, 7.9646e-01, 5.8626e-01, 8.1012e-01, 5.6818e-01, 8.2290e-01,\n",
       "        5.5048e-01, 8.3485e-01, 5.3317e-01, 8.4601e-01, 5.1625e-01, 8.5644e-01,\n",
       "        4.9974e-01, 8.6618e-01, 4.8364e-01, 8.7527e-01, 4.6795e-01, 8.8376e-01,\n",
       "        4.5267e-01, 8.9168e-01, 4.3781e-01, 8.9907e-01, 4.2335e-01, 9.0596e-01,\n",
       "        4.0931e-01, 9.1240e-01, 3.9567e-01, 9.1839e-01, 3.8242e-01, 9.2399e-01,\n",
       "        3.6957e-01, 9.2920e-01, 3.5711e-01, 9.3406e-01, 3.4503e-01, 9.3859e-01,\n",
       "        3.3332e-01, 9.4281e-01, 3.2197e-01, 9.4675e-01, 3.1098e-01, 9.5042e-01,\n",
       "        3.0034e-01, 9.5383e-01, 2.9004e-01, 9.5701e-01, 2.8008e-01, 9.5998e-01,\n",
       "        2.7043e-01, 9.6274e-01, 2.6110e-01, 9.6531e-01, 2.5208e-01, 9.6771e-01,\n",
       "        2.4336e-01, 9.6994e-01, 2.3492e-01, 9.7201e-01, 2.2677e-01, 9.7395e-01,\n",
       "        2.1889e-01, 9.7575e-01, 2.1127e-01, 9.7743e-01, 2.0391e-01, 9.7899e-01,\n",
       "        1.9680e-01, 9.8044e-01, 1.8993e-01, 9.8180e-01, 1.8330e-01, 9.8306e-01,\n",
       "        1.7689e-01, 9.8423e-01, 1.7070e-01, 9.8532e-01, 1.6473e-01, 9.8634e-01,\n",
       "        1.5896e-01, 9.8729e-01, 1.5338e-01, 9.8817e-01, 1.4801e-01, 9.8899e-01,\n",
       "        1.4281e-01, 9.8975e-01, 1.3780e-01, 9.9046e-01, 1.3296e-01, 9.9112e-01,\n",
       "        1.2829e-01, 9.9174e-01, 1.2378e-01, 9.9231e-01, 1.1942e-01, 9.9284e-01,\n",
       "        1.1522e-01, 9.9334e-01, 1.1117e-01, 9.9380e-01, 1.0725e-01, 9.9423e-01,\n",
       "        1.0348e-01, 9.9463e-01, 9.9833e-02, 9.9500e-01, 9.6317e-02, 9.9535e-01,\n",
       "        9.2923e-02, 9.9567e-01, 8.9648e-02, 9.9597e-01, 8.6488e-02, 9.9625e-01,\n",
       "        8.3439e-02, 9.9651e-01, 8.0497e-02, 9.9675e-01, 7.7658e-02, 9.9698e-01,\n",
       "        7.4919e-02, 9.9719e-01, 7.2276e-02, 9.9738e-01, 6.9726e-02, 9.9757e-01,\n",
       "        6.7266e-02, 9.9774e-01, 6.4893e-02, 9.9789e-01, 6.2602e-02, 9.9804e-01,\n",
       "        6.0393e-02, 9.9817e-01, 5.8261e-02, 9.9830e-01, 5.6204e-02, 9.9842e-01,\n",
       "        5.4220e-02, 9.9853e-01, 5.2306e-02, 9.9863e-01, 5.0459e-02, 9.9873e-01,\n",
       "        4.8678e-02, 9.9881e-01, 4.6959e-02, 9.9890e-01, 4.5300e-02, 9.9897e-01,\n",
       "        4.3701e-02, 9.9904e-01, 4.2157e-02, 9.9911e-01, 4.0668e-02, 9.9917e-01,\n",
       "        3.9232e-02, 9.9923e-01, 3.7846e-02, 9.9928e-01, 3.6509e-02, 9.9933e-01,\n",
       "        3.5220e-02, 9.9938e-01, 3.3976e-02, 9.9942e-01, 3.2775e-02, 9.9946e-01,\n",
       "        3.1618e-02, 9.9950e-01, 3.0501e-02, 9.9953e-01, 2.9423e-02, 9.9957e-01,\n",
       "        2.8384e-02, 9.9960e-01, 2.7381e-02, 9.9963e-01, 2.6413e-02, 9.9965e-01,\n",
       "        2.5480e-02, 9.9968e-01, 2.4580e-02, 9.9970e-01, 2.3712e-02, 9.9972e-01,\n",
       "        2.2874e-02, 9.9974e-01, 2.2066e-02, 9.9976e-01, 2.1286e-02, 9.9977e-01,\n",
       "        2.0534e-02, 9.9979e-01, 1.9808e-02, 9.9980e-01, 1.9108e-02, 9.9982e-01,\n",
       "        1.8433e-02, 9.9983e-01, 1.7782e-02, 9.9984e-01, 1.7154e-02, 9.9985e-01,\n",
       "        1.6547e-02, 9.9986e-01, 1.5963e-02, 9.9987e-01, 1.5399e-02, 9.9988e-01,\n",
       "        1.4855e-02, 9.9989e-01, 1.4330e-02, 9.9990e-01, 1.3823e-02, 9.9990e-01,\n",
       "        1.3335e-02, 9.9991e-01, 1.2864e-02, 9.9992e-01, 1.2409e-02, 9.9992e-01,\n",
       "        1.1971e-02, 9.9993e-01, 1.1548e-02, 9.9993e-01, 1.1140e-02, 9.9994e-01,\n",
       "        1.0746e-02, 9.9994e-01, 1.0366e-02, 9.9995e-01, 9.9998e-03, 9.9995e-01,\n",
       "        9.6465e-03, 9.9995e-01, 9.3056e-03, 9.9996e-01, 8.9768e-03, 9.9996e-01,\n",
       "        8.6595e-03, 9.9996e-01, 8.3535e-03, 9.9997e-01, 8.0583e-03, 9.9997e-01,\n",
       "        7.7736e-03, 9.9997e-01, 7.4989e-03, 9.9997e-01, 7.2339e-03, 9.9997e-01,\n",
       "        6.9782e-03, 9.9998e-01, 6.7317e-03, 9.9998e-01, 6.4938e-03, 9.9998e-01,\n",
       "        6.2643e-03, 9.9998e-01, 6.0429e-03, 9.9998e-01, 5.8294e-03, 9.9998e-01,\n",
       "        5.6234e-03, 9.9998e-01, 5.4247e-03, 9.9999e-01, 5.2330e-03, 9.9999e-01,\n",
       "        5.0480e-03, 9.9999e-01, 4.8697e-03, 9.9999e-01, 4.6976e-03, 9.9999e-01,\n",
       "        4.5316e-03, 9.9999e-01, 4.3714e-03, 9.9999e-01, 4.2170e-03, 9.9999e-01,\n",
       "        4.0679e-03, 9.9999e-01, 3.9242e-03, 9.9999e-01, 3.7855e-03, 9.9999e-01,\n",
       "        3.6517e-03, 9.9999e-01, 3.5227e-03, 9.9999e-01, 3.3982e-03, 9.9999e-01,\n",
       "        3.2781e-03, 9.9999e-01, 3.1623e-03, 9.9999e-01, 3.0505e-03, 1.0000e+00,\n",
       "        2.9427e-03, 1.0000e+00, 2.8387e-03, 1.0000e+00, 2.7384e-03, 1.0000e+00,\n",
       "        2.6416e-03, 1.0000e+00, 2.5483e-03, 1.0000e+00, 2.4582e-03, 1.0000e+00,\n",
       "        2.3714e-03, 1.0000e+00, 2.2876e-03, 1.0000e+00, 2.2067e-03, 1.0000e+00,\n",
       "        2.1287e-03, 1.0000e+00, 2.0535e-03, 1.0000e+00, 1.9810e-03, 1.0000e+00,\n",
       "        1.9110e-03, 1.0000e+00, 1.8434e-03, 1.0000e+00, 1.7783e-03, 1.0000e+00,\n",
       "        1.7154e-03, 1.0000e+00, 1.6548e-03, 1.0000e+00, 1.5963e-03, 1.0000e+00,\n",
       "        1.5399e-03, 1.0000e+00, 1.4855e-03, 1.0000e+00, 1.4330e-03, 1.0000e+00,\n",
       "        1.3824e-03, 1.0000e+00, 1.3335e-03, 1.0000e+00, 1.2864e-03, 1.0000e+00,\n",
       "        1.2409e-03, 1.0000e+00, 1.1971e-03, 1.0000e+00, 1.1548e-03, 1.0000e+00,\n",
       "        1.1140e-03, 1.0000e+00, 1.0746e-03, 1.0000e+00, 1.0366e-03, 1.0000e+00,\n",
       "        1.0000e-03, 1.0000e+00, 9.6466e-04, 1.0000e+00, 9.3057e-04, 1.0000e+00,\n",
       "        8.9769e-04, 1.0000e+00, 8.6596e-04, 1.0000e+00, 8.3536e-04, 1.0000e+00,\n",
       "        8.0584e-04, 1.0000e+00, 7.7736e-04, 1.0000e+00, 7.4989e-04, 1.0000e+00,\n",
       "        7.2339e-04, 1.0000e+00, 6.9783e-04, 1.0000e+00, 6.7317e-04, 1.0000e+00,\n",
       "        6.4938e-04, 1.0000e+00, 6.2643e-04, 1.0000e+00, 6.0430e-04, 1.0000e+00,\n",
       "        5.8294e-04, 1.0000e+00, 5.6234e-04, 1.0000e+00, 5.4247e-04, 1.0000e+00,\n",
       "        5.2330e-04, 1.0000e+00, 5.0481e-04, 1.0000e+00, 4.8697e-04, 1.0000e+00,\n",
       "        4.6976e-04, 1.0000e+00, 4.5316e-04, 1.0000e+00, 4.3714e-04, 1.0000e+00,\n",
       "        4.2170e-04, 1.0000e+00, 4.0679e-04, 1.0000e+00, 3.9242e-04, 1.0000e+00,\n",
       "        3.7855e-04, 1.0000e+00, 3.6517e-04, 1.0000e+00, 3.5227e-04, 1.0000e+00,\n",
       "        3.3982e-04, 1.0000e+00, 3.2781e-04, 1.0000e+00, 3.1623e-04, 1.0000e+00,\n",
       "        3.0505e-04, 1.0000e+00, 2.9427e-04, 1.0000e+00, 2.8387e-04, 1.0000e+00,\n",
       "        2.7384e-04, 1.0000e+00, 2.6416e-04, 1.0000e+00, 2.5483e-04, 1.0000e+00,\n",
       "        2.4582e-04, 1.0000e+00, 2.3714e-04, 1.0000e+00, 2.2876e-04, 1.0000e+00,\n",
       "        2.2067e-04, 1.0000e+00, 2.1288e-04, 1.0000e+00, 2.0535e-04, 1.0000e+00,\n",
       "        1.9810e-04, 1.0000e+00, 1.9110e-04, 1.0000e+00, 1.8434e-04, 1.0000e+00,\n",
       "        1.7783e-04, 1.0000e+00, 1.7154e-04, 1.0000e+00, 1.6548e-04, 1.0000e+00,\n",
       "        1.5963e-04, 1.0000e+00, 1.5399e-04, 1.0000e+00, 1.4855e-04, 1.0000e+00,\n",
       "        1.4330e-04, 1.0000e+00, 1.3824e-04, 1.0000e+00, 1.3335e-04, 1.0000e+00,\n",
       "        1.2864e-04, 1.0000e+00, 1.2409e-04, 1.0000e+00, 1.1971e-04, 1.0000e+00,\n",
       "        1.1548e-04, 1.0000e+00, 1.1140e-04, 1.0000e+00, 1.0746e-04, 1.0000e+00,\n",
       "        1.0366e-04, 1.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(12, 512)\n",
    "c = pe(y)[0]\n",
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(torch.all(c[i] == x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
